{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\hurin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "minmax_scaler = MinMaxScaler()\n",
    "maxabs_scaler = MaxAbsScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Baseline Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.90      0.85      2926\n",
      "           0       0.50      0.22      0.30       502\n",
      "           1       0.64      0.60      0.62      1073\n",
      "\n",
      "    accuracy                           0.75      4501\n",
      "   macro avg       0.64      0.57      0.59      4501\n",
      "weighted avg       0.73      0.75      0.73      4501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5879760408201458"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "X_train = train['Text']\n",
    "y_train = train['Verdict']\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the BoW Vectorizer\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "# Fit and transform the training data, and transform the testing data\n",
    "X_training_bow = bow_vectorizer.fit_transform(X_training)\n",
    "X_validation_bow = bow_vectorizer.transform(X_validation)\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_training_bow, y_training)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_prediction = nb_classifier.predict(X_validation_bow)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_validation, y_prediction))\n",
    "\n",
    "f1_score(y_validation, y_prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('test.csv')\n",
    "X_test = test['Text']\n",
    "\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "result = nb_classifier.predict(X_test_bow)\n",
    "\n",
    "test['Verdict'] = pd.Series(result)\n",
    "test.drop(columns=['Text'], inplace=True)\n",
    "test.to_csv('A0233573E_Naive_Bayes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Improvising the Model with Data Pre-processing</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset engineering: Remove empty sentences, duplicate sentences and identical sentences with conflicting verdict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(train):\n",
    "    # Remove rows where 'Text' is '#NAME'\n",
    "    train = train[train['Text'] != '#NAME']\n",
    "    train = train[train['Text'] != '#NAME?']\n",
    "\n",
    "    # Remove duplicate sentences with same verdict label\n",
    "    train = train.drop_duplicates(subset=['Text', 'Verdict'])\n",
    "\n",
    "    # Identify identical sentences\n",
    "    duplicates_all = train[train.duplicated('Text', keep=False)]\n",
    "\n",
    "    # Filter out sentences with more than one unique 'Verdict' value\n",
    "    conflicting = duplicates_all.groupby('Text').filter(lambda x: x['Verdict'].nunique() > 1)\n",
    "\n",
    "    # Find the unique texts that have conflicting verdicts\n",
    "    conflicting_texts = conflicting['Text'].unique()\n",
    "\n",
    "    # Remove all instances of these sentences from the dataset\n",
    "    train = train[~train['Text'].isin(conflicting_texts)]\n",
    "\n",
    "    # Sort by 'Text' for better readability\n",
    "    train = train.sort_values('Text').reset_index(drop=True)\n",
    "\n",
    "    # Save the cleaned dataset\n",
    "    train.to_csv('after dataset preprocessing.csv', index=False)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess training text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text) #NLTK\n",
    "\n",
    "    # document = nlp(text) #SpaCy\n",
    "    # tokens = [token.text for token in document]\n",
    "    \n",
    "    # Remove stop words\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # stemming\n",
    "    # stemmer = PorterStemmer()\n",
    "    # tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # lemmatization\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    # tokens= [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # Re-join tokens into a single string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "    # preprocessed_text = text\n",
    "    \n",
    "    return preprocessed_text\n",
    "    # return tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation: Create more sentences with synonym replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(sentence, replacement_rate=0.1):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if random.random() < replacement_rate:\n",
    "            synonyms = [lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name() != word]\n",
    "            if synonyms:\n",
    "                new_words.append(random.choice(synonyms))\n",
    "                continue\n",
    "        new_words.append(word)\n",
    "    return ' '.join(new_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation: Create more sentences by adding noise (typos) to existing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_typo_noise(sentence, noise_rate=0.05):\n",
    "    letters = list(sentence)\n",
    "    for i in range(len(letters)):\n",
    "        if random.random() < noise_rate and letters[i].isalpha():\n",
    "            swap_index = i + random.choice([-1, 1])  # Swap with the previous or next character\n",
    "            if 0 <= swap_index < len(letters):\n",
    "                letters[i], letters[swap_index] = letters[swap_index], letters[i]\n",
    "    return ''.join(letters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentences(train):\n",
    "\n",
    "    replacement_rate = 0.1 # for synonym replacement\n",
    "    noise_rate = 0.05 # for adding typos\n",
    "\n",
    "    # Count sentences per verdict category\n",
    "    verdict_counts = train['Verdict'].value_counts()\n",
    "    target_count = verdict_counts.max()\n",
    "\n",
    "    augmented_sentences = []\n",
    "    augmented_verdicts = []\n",
    "\n",
    "    for verdict, count in train['Verdict'].value_counts().items():\n",
    "        # If this verdict category is already at or above the target, just continue\n",
    "        if count >= target_count:\n",
    "            continue\n",
    "\n",
    "        # Calculate how many additional sentences are needed\n",
    "        additional_needed = target_count - count\n",
    "\n",
    "        # Filter the DataFrame for the current verdict category\n",
    "        verdict_df = train[train['Verdict'] == verdict]\n",
    "\n",
    "        # While loop to keep augmenting until we reach the required additional count\n",
    "        while additional_needed > 0:\n",
    "            for _, row in verdict_df.iterrows():\n",
    "                if additional_needed <= 0:\n",
    "                    break  # Break if no more sentences are needed\n",
    "                # augmented_sentence = synonym_replacement(row['Text'], replacement_rate)\n",
    "                augmented_sentence = add_typo_noise(row['Text'], noise_rate)\n",
    "                # Avoid adding the exact original sentence\n",
    "                if augmented_sentence != row['Text']:\n",
    "                    augmented_sentences.append(augmented_sentence)\n",
    "                    augmented_verdicts.append(verdict)\n",
    "                    additional_needed -= 1\n",
    "\n",
    "    # Append the new sentences to the original DataFrame\n",
    "    augmented_df = pd.DataFrame({'Text': augmented_sentences, 'Verdict': augmented_verdicts})\n",
    "    return pd.concat([train, augmented_df], ignore_index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Improvising the Model with Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence length and number of each type of POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer_nltk(X_training):\n",
    "\n",
    "    list_of_features = []\n",
    "\n",
    "    pos_tags_of_interest = ['NN', 'VB', 'JJ', 'RB']  # nouns, verbs, adjectives, adverbs\n",
    "\n",
    "    for sentence in X_training:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Get POS tags for the tokens in the sentence\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Count the occurrences of each POS tag of interest\n",
    "        pos_counts = {tag: 0 for tag in pos_tags_of_interest}\n",
    "        for word, tag in pos_tags:\n",
    "            if tag in pos_tags_of_interest:\n",
    "                pos_counts[tag] += 1\n",
    "        \n",
    "        # Calculate sentence length\n",
    "        sentence_length = len(tokens)\n",
    "\n",
    "        # Combine sentence length and POS tag counts into a single feature array for this sentence\n",
    "        features = [sentence_length] + [pos_counts[tag] for tag in pos_tags_of_interest]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineer_spacy(X_training):\n",
    "\n",
    "    pos_tags_of_interest = ['NOUN', 'VERB', 'ADJ', 'ADV']  # nouns, verbs, adjectives, adverbs\n",
    "    \n",
    "    features_list = []\n",
    "\n",
    "    for doc in nlp.pipe(X_training):\n",
    "        # Count the occurrences of each POS tag of interest\n",
    "        pos_counts = {tag: 0 for tag in pos_tags_of_interest}\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_tags_of_interest:\n",
    "                pos_counts[token.pos_] += 1\n",
    "        \n",
    "        # Calculate sentence length\n",
    "        sentence_length = len(doc)\n",
    "\n",
    "        # Combine sentence length and POS tag counts into a single feature array for this sentence\n",
    "        features = [sentence_length] + [pos_counts[tag] for tag in pos_tags_of_interest]\n",
    "        features_list.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(features_list)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_textblob(X_training):\n",
    "    \n",
    "    list_of_features = []\n",
    "\n",
    "    for sentence in X_training:\n",
    "\n",
    "        # Analyze sentiment with TextBlob\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_polarity = blob.sentiment.polarity\n",
    "        sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "        # Combine sentence length, POS tag counts, and sentiment features into a single feature array for this sentence\n",
    "        features = [sentiment_polarity, sentiment_subjectivity]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis_vader(X_training):\n",
    "    list_of_features = []\n",
    "\n",
    "    for sentence in X_training:\n",
    "        # Apply VADER sentiment analysis\n",
    "        vader_scores = sia.polarity_scores(sentence)\n",
    "\n",
    "        # Extract the scores for positive, negative, neutral, and compound sentiments\n",
    "        vader_positive = vader_scores['pos']\n",
    "        vader_negative = vader_scores['neg']\n",
    "        vader_neutral = vader_scores['neu']\n",
    "        vader_compound = vader_scores['compound']\n",
    "\n",
    "        # Combine the VADER sentiment scores into a single feature array for this sentence\n",
    "        features = [vader_positive, vader_negative, vader_neutral, vader_compound]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique words to sentence length ratio with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_ratio_nltk(X_training):\n",
    "    list_of_features = []\n",
    "\n",
    "    for sentence in X_training:\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Calculate unique words vs sentence length ratio\n",
    "        unique_words_ratio = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "        \n",
    "        # Combine the ratio into the feature array for this sentence\n",
    "        features = [unique_words_ratio]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique words to sentence length ratio with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_words_ratio_spacy(X_training):\n",
    "    list_of_features = []\n",
    "\n",
    "    for doc in nlp.pipe(X_training):\n",
    "        # Calculate unique words vs sentence length ratio\n",
    "        unique_words_ratio = len(set(token.text for token in doc)) / len(doc) if doc else 0\n",
    "        \n",
    "        # Combine the ratio into the feature array for this sentence\n",
    "        features = [unique_words_ratio]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with Textblob + Finding ratio of unique words to length of sentence with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textblob_and_unique_words_nltk(X_training):\n",
    "    \n",
    "    list_of_features = []\n",
    "\n",
    "    for sentence in X_training:\n",
    "\n",
    "        # Analyze sentiment with TextBlob\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_polarity = blob.sentiment.polarity\n",
    "        sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Calculate unique words vs sentence length ratio\n",
    "        unique_words_ratio = len(set(tokens)) / len(tokens) if tokens else 0\n",
    "        \n",
    "        # Combine the ratio into the feature array for this sentence\n",
    "        features = [sentiment_polarity, sentiment_subjectivity] + [unique_words_ratio]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Main Code!</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.80      0.91      0.85      2926\n",
      "           0       0.49      0.20      0.29       502\n",
      "           1       0.66      0.59      0.62      1073\n",
      "\n",
      "    accuracy                           0.76      4501\n",
      "   macro avg       0.65      0.57      0.59      4501\n",
      "weighted avg       0.73      0.76      0.73      4501\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5873756107527343"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# train = preprocess_dataset(train)\n",
    "\n",
    "X_train = train['Text']\n",
    "y_train = train['Verdict']\n",
    "\n",
    "# Apply augmentation\n",
    "# train_augmented = augment_sentences(train)\n",
    "\n",
    "# X_train_augmented = train_augmented['Text']\n",
    "# y_train_augmented = train_augmented['Verdict']\n",
    "\n",
    "X_train_preprocessed = X_train.apply(preprocess_text)\n",
    "# X_train_preprocessed = X_train_augmented.apply(preprocess_text)\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)\n",
    "# X_training, X_validation, y_training, y_validation = train_test_split(X_train_preprocessed, y_train_augmented, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the BoW Vectorizer\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "# tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the training data, and transform the testing data\n",
    "X_training_bow = bow_vectorizer.fit_transform(X_training)\n",
    "X_validation_bow = bow_vectorizer.transform(X_validation)\n",
    "\n",
    "# X_training_tfidf = tfidf_vectorizer.fit_transform(X_training)\n",
    "# X_validation_tfidf = tfidf_vectorizer.transform(X_validation)\n",
    "\n",
    "# MAX ABS\n",
    "# Fit and transform the training data\n",
    "# X_training_bow_scaled = maxabs_scaler.fit_transform(X_training_bow.toarray())\n",
    "# Only need to transform validation data\n",
    "# X_validation_bow_scaled = maxabs_scaler.transform(X_validation_bow.toarray())\n",
    "\n",
    "# MIN MAX\n",
    "# Fit and transform the training data\n",
    "# X_training_bow_scaled = minmax_scaler.fit_transform(X_training_bow.toarray())\n",
    "# Only need to transform validation data\n",
    "# X_validation_bow_scaled = minmax_scaler.transform(X_validation_bow.toarray())\n",
    "\n",
    "# X_training_engineered = feature_engineer_nltk(X_training)\n",
    "# X_training_engineered = feature_engineer_spacy(X_training)\n",
    "# X_training_engineered = sentiment_analysis_textblob(X_training)\n",
    "# X_training_engineered = sentiment_analysis_vader(X_training)\n",
    "# X_training_engineered = unique_words_ratio_nltk(X_training)\n",
    "# X_training_engineered = unique_words_ratio_spacy(X_training)\n",
    "X_training_engineered = textblob_and_unique_words_nltk(X_training)\n",
    "X_training_combined = np.hstack([X_training_bow.toarray(), X_training_engineered])\n",
    "\n",
    "# X_validation_engineered = feature_engineer_nltk(X_validation)\n",
    "# X_validation_engineered = feature_engineer_spacy(X_validation)\n",
    "# X_validation_engineered = sentiment_analysis_textblob(X_validation)\n",
    "# X_validation_engineered = sentiment_analysis_vader(X_validation)\n",
    "# X_validation_engineered = unique_words_ratio_nltk(X_validation)\n",
    "# X_validation_engineered = unique_words_ratio_spacy(X_validation)\n",
    "X_validation_engineered = textblob_and_unique_words_nltk(X_validation)\n",
    "X_validation_combined = np.hstack([X_validation_bow.toarray(), X_validation_engineered])\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier.fit(X_training_combined, y_training)\n",
    "# nb_classifier.fit(X_training_bow, y_training)\n",
    "# nb_classifier.fit(X_training_bow_scaled, y_training)\n",
    "# nb_classifier.fit(X_training_tfidf, y_training)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_prediction = nb_classifier.predict(X_validation_combined)\n",
    "# y_prediction = nb_classifier.predict(X_validation_bow)\n",
    "# y_prediction = nb_classifier.predict(X_validation_bow_scaled)\n",
    "# y_prediction = nb_classifier.predict(X_validation_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_validation, y_prediction))\n",
    "\n",
    "f1_score(y_validation, y_prediction, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Don't preprocess testing dataset\n",
    "# test = preprocess_dataset(test)\n",
    "\n",
    "X_test = test['Text']\n",
    "\n",
    "X_test_preprocessed = X_test.apply(preprocess_text)\n",
    "\n",
    "X_test_bow = bow_vectorizer.transform(X_test_preprocessed)\n",
    "# X_test_bow_scaled = maxabs_scaler.transform(X_test_bow.toarray())\n",
    "# X_test_bow_scaled = minmax_scaler.transform(X_test_bow.toarray())\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# X_test_engineered = feature_engineer_nltk(X_test)\n",
    "# X_test_engineered = feature_engineer_spacy(X_test)\n",
    "# X_test_engineered = sentiment_analysis_textblob(X_test)\n",
    "# X_test_engineered = sentiment_analysis_vader(X_test)\n",
    "# X_test_engineered = unique_words_ratio_nltk(X_test)\n",
    "# X_test_engineered = unique_words_ratio_spacy(X_test)\n",
    "X_test_engineered = textblob_and_unique_words_nltk(X_test)\n",
    "X_test_combined = np.hstack([X_test_bow.toarray(), X_test_engineered])\n",
    "\n",
    "result = nb_classifier.predict(X_test_combined)\n",
    "# result = nb_classifier.predict(X_test_bow)\n",
    "# result = nb_classifier.predict(X_test_bow_scaled)\n",
    "# result = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "test['Verdict'] = pd.Series(result)\n",
    "test.drop(columns=['Text'], inplace=True)\n",
    "test.to_csv('A0233573E_Naive_Bayes_with lowercase.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
