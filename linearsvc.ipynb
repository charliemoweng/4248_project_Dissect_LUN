{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train LinearSVC model on data and analyse using ELI5\n",
    "sources: \\\n",
    "https://medium.com/@gaurishah143/xg-boost-for-text-classification-9c8b1f8f24aa \\\n",
    "https://github.com/salonipriyani/eli5-article/blob/main/NLP-eli5.ipynb \\\n",
    "https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html \\\n",
    "\\\n",
    "About LinearSVC:\n",
    "- stands for Linear Support Vector Classification\n",
    "- Both LinearSVC and SVC of sklearn are based on Support Vector Machine (SVM)\n",
    "- sklearn documentation on LinearSVC: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "- on SVM: https://www.ibm.com/topics/support-vector-machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.sparse import hstack, csr_matrix, vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data and labels from original dataset\n",
    "data_original = pd.read_csv('dataset/fulltrain.csv')  # pandas DataFrame\n",
    "texts = data_original['Text']  # pandas Series\n",
    "labels = data_original['Label']\n",
    "# use the balancedtest file. comment out if using fulltrain file.\n",
    "test_data = pd.read_csv('dataset/balancedtest.csv')\n",
    "texts_test = test_data['Text']\n",
    "labels_test = test_data['Label']\n",
    "\n",
    "# # Text data and labels from augmented dataset\n",
    "# data_augmented = pd.read_csv('dataset/merged_final_df_with_topics_new.csv')\n",
    "# texts = data_augmented['text']\n",
    "# labels = data_augmented['label']\n",
    "# # for augmented dataset\n",
    "# test_data = pd.read_csv('dataset/test_final_with_topics_new.csv')\n",
    "# texts_test = test_data['text']\n",
    "# labels_test = test_data['label']\n",
    "\n",
    "# # Split the data into train and test sets, when running tests on partition of training corpus.\n",
    "# texts_train, texts_test, labels_train, labels_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# when using the entire corpus as training data\n",
    "texts_train = texts\n",
    "labels_train = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing & Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train dataset: 48854\n",
      "Number of rows in test dataset: 3000\n",
      "Number of features in train dataset: 7388647\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows of original data using the shape attribute\n",
    "num_rows_train = data_original.shape[0]\n",
    "print(\"Number of rows in train dataset:\", num_rows_train)\n",
    "num_rows_test = test_data.shape[0]\n",
    "print(\"Number of rows in test dataset:\", num_rows_test)\n",
    "\n",
    "# # Check the number of rows of augmented data using the shape attribute\n",
    "# num_rows_train = data_augmented.shape[0]\n",
    "# print(\"Number of rows in train dataset:\", num_rows_train)\n",
    "# num_rows_test = test_data.shape[0]\n",
    "# print(\"Number of rows in test dataset:\", num_rows_test)\n",
    "\n",
    "vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "vectorizer_fit = vec.fit(texts_train)\n",
    "\n",
    "texts_train = vectorizer_fit.transform(texts_train)  # becomes a csr_matrix (sparse matrix)\n",
    "\n",
    "# Get the feature names (terms)\n",
    "train_feature_names = vectorizer_fit.get_feature_names_out()\n",
    "# Get the number of features (terms)\n",
    "num_features = len(train_feature_names)\n",
    "print(\"Number of features in train dataset:\", num_features)\n",
    "\n",
    "texts_test = vectorizer_fit.transform(texts_test)\n",
    "\n",
    "\n",
    "# # incorporate extra features (columns) in augmented dataset\n",
    "# encoded_df = pd.get_dummies(data_augmented, columns = ['has_swear_word', 'severity', 'topic'])\n",
    "# dropped_df = encoded_df.drop(['label', 'text', 'processed_text'], axis=1)\n",
    "# sparse = csr_matrix(dropped_df)\n",
    "# sparse_2 = csr_matrix(texts_train)\n",
    "# texts_train_combined = hstack([sparse_2, sparse])\n",
    "\n",
    "# encoded_df = pd.get_dummies(test_data, columns = ['has_swear_word', 'severity', 'topic'])\n",
    "# dropped_df = encoded_df.drop(['label', 'text', 'processed_text'], axis=1)\n",
    "# sparse = csr_matrix(dropped_df)\n",
    "# sparse_2 = csr_matrix(texts_test)\n",
    "# texts_test_combined = hstack([sparse_2, sparse])\n",
    "\n",
    "# def pad_columns(matrix1, matrix2):\n",
    "#     matrix1_rows = matrix1.shape[0]\n",
    "#     matrix2_rows = matrix2.shape[0]\n",
    "#     matrix1_cols = matrix1.shape[1]\n",
    "#     matrix2_cols = matrix2.shape[1]\n",
    "#     diff = matrix1_cols - matrix2_cols\n",
    "#     if (diff < 0):\n",
    "#         # Need to pad columns to matrix 1\n",
    "#         diff = diff * -1\n",
    "#         zero_matrix = csr_matrix((matrix1_rows, diff))\n",
    "#         matrix1 = hstack([matrix1, zero_matrix])\n",
    "#     elif (diff > 0):\n",
    "#         # Need to pad columns to matrix 2\n",
    "#         zero_matrix = csr_matrix((matrix2_rows, diff))\n",
    "#         matrix2 = hstack([matrix2, zero_matrix])\n",
    "#     return (matrix1, matrix2)\n",
    "\n",
    "# texts_train, texts_test = pad_columns(texts_train_combined, texts_test_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# params = {\n",
    "#     'C': [0.1, 1.0, 10.0], # C=10.0 is the best\n",
    "#     'penalty': ['l2'], # default value\n",
    "#     'loss': ['squared_hinge'], # default value\n",
    "#     'dual': [True] # default value\n",
    "# }\n",
    "\n",
    "# # Hyper-parameters fine tuning\n",
    "# grid_search = GridSearchCV(estimator=LinearSVC(), param_grid=params, cv=5, verbose=2) # cv means cross-validation\n",
    "# grid_search.fit(texts_train, labels_train)\n",
    "# best_params = grid_search.best_params_\n",
    "# print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.7576666666666667\n",
      "f1 score:  0.7524138201939159\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.74      0.81       750\n",
      "           2       0.77      0.54      0.63       750\n",
      "           3       0.65      0.81      0.72       750\n",
      "           4       0.77      0.95      0.85       750\n",
      "\n",
      "    accuracy                           0.76      3000\n",
      "   macro avg       0.77      0.76      0.75      3000\n",
      "weighted avg       0.77      0.76      0.75      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the model on the training data\n",
    "svm = LinearSVC(C=10.0)\n",
    "\n",
    "clf = CalibratedClassifierCV(svm)  # add in this for predict_proba, reference: https://stackoverflow.com/questions/26478000/converting-linearsvcs-decision-function-to-probabilities-scikit-learn-python\n",
    "clf.fit(texts_train, labels_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = clf.predict(texts_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(labels_test, predictions)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "f1_score = f1_score(labels_test, predictions, average='macro')\n",
    "print(\"f1 score: \", f1_score)\n",
    "\n",
    "print(classification_report(labels_test, predictions, labels=[1, 2, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # output prediction results for original dataset in csv for further analysis\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Text': test_data['Text'],\n",
    "#     'Original Label': labels_test,\n",
    "#     'Predicted Label': predictions\n",
    "# })\n",
    "# results_df.to_csv('dataset/linearsvc_model_predictions_original_dataset.csv', index=False)\n",
    "\n",
    "# # output prediction results for augmented dataset in csv for further analysis\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Text': test_data['text'],\n",
    "#     'Original Label': labels_test,\n",
    "#     'Predicted Label': predictions\n",
    "# })\n",
    "# results_df.to_csv('dataset/linearsvc_model_predictions_augmented_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.536 Satire\n",
      "0.270 Hoax\n",
      "0.158 Propaganda\n",
      "0.035 Reliable News\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "def monkeypath_itemfreq(sampler_indices):\n",
    "   return zip(*np.unique(sampler_indices, return_counts=True))\n",
    "\n",
    "scipy.stats.itemfreq=monkeypath_itemfreq\n",
    "\n",
    "import eli5\n",
    "from eli5 import explain_weights, explain_prediction\n",
    "\n",
    "# doc = test_data['Text'][0]\n",
    "# print(doc)\n",
    "doc = \"Currently ELI5 allows to explain weights and predictions of scikit-learn linear classifiers and regressors, print decision trees as text or as SVG, show feature importances and explain predictions of decision trees and tree-based ensembles.\"\n",
    "labels = ['Satire', 'Hoax', 'Propaganda', 'Reliable News']\n",
    "\n",
    "def print_prediction(doc):\n",
    "    doc_transformed = vec.transform([doc])[0]\n",
    "    y_pred = clf.predict_proba(doc_transformed)\n",
    "    for target, prob in zip(labels, y_pred[0]):\n",
    "        print(\"{:.3f} {}\".format(prob, target))\n",
    "\n",
    "print_prediction(doc)\n",
    "\n",
    "# from eli5.lime import TextExplainer\n",
    "\n",
    "# # doc_2d = np.reshape(doc, (-1, 1))  # change 1D array to 2D array. still does not work.\n",
    "# te = TextExplainer(random_state=42)\n",
    "# te.fit(doc, clf.predict_proba)  # using doc, error msg: ValueError: Expected 2D array, got 1D array instead. using doc_2d, error msg: AttributeError: 'numpy.ndarray' object has no attribute 'lower'\n",
    "# te.show_prediction(target_names=labels)\n",
    "\n",
    "# # shows results but not useful as \"features\" are just hex that represent the words\n",
    "# eli5.show_prediction(clf, test_data['text'][0], target_names=['Satire', 'Hoax', 'Propaganda', 'Reliable News'], vec=vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "4248_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
