{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting done\n",
      "Converted sentences to vectors\n",
      "Converted vectors to tensors\n",
      "Epoch\tLoss\t\n",
      "-----\t----\n",
      "   20\t0.8044177889823914\n",
      "   40\t0.7019259929656982\n",
      "   60\t0.6268673539161682\n",
      "   80\t0.5829234719276428\n",
      "  100\t0.5463336110115051\n",
      "  120\t0.5127717852592468\n",
      "  140\t0.475117951631546\n",
      "[ 1 -1 -1 ...  0 -1 -1]\n",
      "[-1 -1 -1 ...  0 -1 -1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.82      0.91      0.86      2926\n",
      "           0       0.57      0.30      0.39       502\n",
      "           1       0.67      0.62      0.64      1073\n",
      "\n",
      "    accuracy                           0.77      4501\n",
      "   macro avg       0.69      0.61      0.63      4501\n",
      "weighted avg       0.75      0.77      0.76      4501\n",
      "\n",
      "0.6317390831028654\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_vec(sentence, model):\n",
    "    vecs = []\n",
    "    for word in sentence.split():\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "X_train = train['Text']\n",
    "y_train = train['Verdict']\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Splitting done\")\n",
    "\n",
    "# Map class labels to stert from 0 to fit NN model\n",
    "y_training_mapped = y_training.map({-1: 0, 0: 1, 1: 2})\n",
    "y_validation_mapped = y_validation.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "# bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "# X_training_bow = bow_vectorizer.fit_transform(X_training)\n",
    "# X_validation_bow = bow_vectorizer.transform(X_validation)\n",
    "\n",
    "X_training_to_vec = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_training])\n",
    "X_validation_to_vec = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_validation])\n",
    "print(\"Converted sentences to vectors\")\n",
    "\n",
    "# convert sparse matrices returned by CountVectorizer into dense matrices before converting them into PyTorch tensors\n",
    "# X_training_bow_tensor = torch.FloatTensor(X_training_bow.toarray())\n",
    "X_training_w2v_tensor = torch.FloatTensor(X_training_to_vec)\n",
    "# X_validation_bow_tensor = torch.FloatTensor(X_validation_bow.toarray())\n",
    "X_validation_w2v_tensor = torch.FloatTensor(X_validation_to_vec)\n",
    "\n",
    "y_training_tensor = torch.LongTensor(y_training_mapped)\n",
    "y_validation_tensor = torch.LongTensor(y_validation_mapped.to_numpy())\n",
    "print(\"Converted vectors to tensors\")\n",
    "\n",
    "# num_features = X_training_bow.shape[1]\n",
    "embedding_size = word_vectors.vector_size  # This should be 300 for Google News Word2Vec\n",
    "hidden_size = 512  # Example size, can be adjusted\n",
    "num_classes = len(np.unique(y_training_mapped))  # The number of unique classes in your target variable\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # halving the size\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes) # Output size matches the number of classes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "\n",
    "epochs = 140\n",
    "\n",
    "# nn_model = SimpleNeuralNet()\n",
    "nn_model = SimpleNeuralNet(embedding_size, hidden_size, num_classes, dropout_rate)\n",
    "# the optimiser controls the learning rate\n",
    "# optimiser = torch.optim.SGD(nn_model.parameters(), lr=1e-2, momentum=0)\n",
    "\n",
    "# optimiser with weight decay for L2 regularization\n",
    "# The weight_decay parameter is used to specify the L2 penalty\n",
    "# optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, momentum=0, weight_decay=1e-5) \n",
    "\n",
    "# Adam without L2 regularisation\n",
    "optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "\n",
    "'''\n",
    "The L2 regularization penalty works by adding a term to the loss that penalizes large weights, which helps prevent overfitting. \n",
    "This term is the sum of the squares of all the weights multiplied by the weight_decay value. During backpropagation, \n",
    "this has the effect of shrinking the weights slightly on every update, hence the term \"weight decay.\"\n",
    "\n",
    "Be cautious when combining L2 regularization with the Adam optimizer, as Adam already includes a form of regularization \n",
    "through its moving average of squared gradients (which is similar to RMSprop). Excessive regularization might lead to underfitting, \n",
    "so itâ€™s important to find a balance that works well with your data and model architecture.\n",
    "'''\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs + 1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = nn_model(X_training_w2v_tensor)\n",
    "\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_training_tensor.shape)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y_training_tensor)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "\n",
    "    # Print every 1000 epochs\n",
    "    if i % 20 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    y_prediction_logits = nn_model(X_validation_w2v_tensor)\n",
    "    y_prediction_classes = torch.argmax(y_prediction_logits, dim=1)  # Convert logits to class labels\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_validation_numpy = y_validation_tensor.numpy() - 1\n",
    "y_prediction_numpy = y_prediction_classes.numpy() - 1\n",
    "\n",
    "print(y_validation_numpy)\n",
    "print(y_prediction_numpy)\n",
    "\n",
    "print(classification_report(y_validation_numpy, y_prediction_numpy))\n",
    "print(f1_score(y_validation_numpy, y_prediction_numpy, average='macro'))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "#print(classification_report(y_validation.numpy(), y_prediction.numpy()))\n",
    "\n",
    "#f1_score(y_validation, y_prediction, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('test.csv')\n",
    "X_test = test['Text']\n",
    "\n",
    "# X_test_bow = bow_vectorizer.transform(X_test)\n",
    "X_test_to_vec = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_test])\n",
    "# X_test_bow_tensor = torch.FloatTensor(X_test_bow.toarray())\n",
    "X_test_w2v_tensor = torch.FloatTensor(X_test_to_vec)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # y_testing_logits = nn_model(X_test_bow_tensor)\n",
    "    y_testing_logits = nn_model(X_test_w2v_tensor)\n",
    "    y_testing_classes = torch.argmax(y_testing_logits, dim=1) \n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_test_numpy = y_testing_classes.numpy() - 1\n",
    "\n",
    "test['Verdict'] = pd.Series(y_test_numpy)\n",
    "test.drop(columns=['Text'], inplace=True)\n",
    "test.to_csv('Project simple neural network.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
