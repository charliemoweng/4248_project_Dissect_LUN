{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "minmax_scaler = MinMaxScaler()\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with Textblob + Frequency of each POS tag in a sentence with SpaCy + Sentence length + Ratio of unique words to length of sentence with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_textblob(X_training):\n",
    "    \n",
    "    list_of_features = []\n",
    "\n",
    "    for sentence in X_training:\n",
    "\n",
    "        # Analyze sentiment with TextBlob\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_polarity = blob.sentiment.polarity\n",
    "        sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "        # Combine sentence length, POS tag counts, and sentiment features into a single feature array for this sentence\n",
    "        features = [sentiment_polarity, sentiment_subjectivity]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X_training):\n",
    "\n",
    "    pos_tags_of_interest = ['NOUN', 'VERB', 'ADJ', 'ADV']  # nouns, verbs, adjectives, adverbs\n",
    "    list_of_features = []\n",
    "\n",
    "    for doc in nlp.pipe(X_training):\n",
    "        # Count the occurrences of each POS tag of interest\n",
    "        pos_counts = {tag: 0 for tag in pos_tags_of_interest}\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_tags_of_interest:\n",
    "                pos_counts[token.pos_] += 1\n",
    "\n",
    "        \n",
    "        # Calculate unique words vs sentence length ratio\n",
    "        unique_words_ratio = len(set(token.text for token in doc)) / len(doc) if doc else 0\n",
    "\n",
    "        # Calculate sentence length\n",
    "        sentence_length = len(doc)\n",
    "\n",
    "        # Combine sentence length and POS tag counts into a single feature array for this sentence\n",
    "        features = [unique_words_ratio] + [sentence_length] + [pos_counts[tag] for tag in pos_tags_of_interest]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting done\n",
      "Feature engineering completed\n",
      "Converted sentences to vectors\n",
      "(18000, 300)\n",
      "(18000, 6)\n",
      "(18000, 2)\n",
      "Integrated feature engineering into the dataset\n",
      "Converted vectors to tensors\n",
      "Epoch\tLoss\t\n",
      "-----\t----\n",
      "   10\t0.8464913964271545\n",
      "   20\t0.8250763416290283\n",
      "   30\t0.7739527821540833\n",
      "   40\t0.7273467183113098\n",
      "   50\t0.6923595666885376\n",
      "   60\t0.6534910798072815\n",
      "   70\t0.6211737990379333\n",
      "   80\t0.5962772965431213\n",
      "   90\t0.5782447457313538\n",
      "  100\t0.5616274476051331\n",
      "  110\t0.5434944033622742\n",
      "  120\t0.5298254489898682\n",
      "  130\t0.5128721594810486\n",
      "  140\t0.49789106845855713\n",
      "  150\t0.48027804493904114\n",
      "Validation data evaluated\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.89      0.86      2926\n",
      "           0       0.59      0.29      0.39       502\n",
      "           1       0.65      0.68      0.66      1073\n",
      "\n",
      "    accuracy                           0.77      4501\n",
      "   macro avg       0.69      0.62      0.64      4501\n",
      "weighted avg       0.76      0.77      0.76      4501\n",
      "\n",
      "0.6369800549703427\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_vec(sentence, model):\n",
    "    vecs = []\n",
    "    for word in sentence.split():\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv('train.csv')\n",
    "X_train = train['Text']\n",
    "y_train = train['Verdict']\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Splitting done\")\n",
    "\n",
    "# Map class labels to stert from 0 to fit NN model\n",
    "y_training_mapped = y_training.map({-1: 0, 0: 1, 1: 2})\n",
    "y_validation_mapped = y_validation.map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "# bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "# X_training_bow = bow_vectorizer.fit_transform(X_training)\n",
    "# X_validation_bow = bow_vectorizer.transform(X_validation)\n",
    "\n",
    "X_training_with_sentiment = sentiment_analysis_textblob(X_training)\n",
    "X_validation_with_sentiment = sentiment_analysis_textblob(X_validation)\n",
    "\n",
    "X_training_engineered = feature_engineering(X_training)\n",
    "X_validation_engineered = feature_engineering(X_validation)\n",
    "print(\"Feature engineering completed\")\n",
    "\n",
    "X_training_w2v = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_training])\n",
    "X_validation_w2v = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_validation])\n",
    "print(\"Converted sentences to vectors\")\n",
    "\n",
    "print(X_training_w2v.shape)  # Should be (18000, num_features_w2v)\n",
    "print(X_training_engineered.shape)  # Should be (18000, num_features_engineered)\n",
    "print(X_training_with_sentiment.shape)  # Should be (18000, num_features_sentiment)\n",
    "\n",
    "X_training_combined = np.hstack([X_training_w2v, X_training_engineered, X_training_with_sentiment])\n",
    "X_validation_combined = np.hstack([X_validation_w2v, X_validation_engineered, X_validation_with_sentiment])\n",
    "print(\"Integrated feature engineering into the dataset\")\n",
    "\n",
    "# convert sparse matrices returned by CountVectorizer into dense matrices before converting them into PyTorch tensors\n",
    "# X_training_bow_tensor = torch.FloatTensor(X_training_bow.toarray())\n",
    "# X_training_w2v_tensor = torch.FloatTensor(X_training_w2v)\n",
    "X_training_w2v_tensor = torch.FloatTensor(X_training_combined)\n",
    "\n",
    "# X_validation_bow_tensor = torch.FloatTensor(X_validation_bow.toarray())\n",
    "# X_validation_w2v_tensor = torch.FloatTensor(X_validation_w2v)\n",
    "X_validation_w2v_tensor = torch.FloatTensor(X_validation_combined)\n",
    "\n",
    "y_training_tensor = torch.LongTensor(y_training_mapped)\n",
    "y_validation_tensor = torch.LongTensor(y_validation_mapped.to_numpy())\n",
    "print(\"Converted vectors to tensors\")\n",
    "\n",
    "# num_features = X_training_bow.shape[1]\n",
    "embedding_size = word_vectors.vector_size + 8  # This should be 300 for Google News Word2Vec + 8 additional features\n",
    "hidden_size = 512\n",
    "num_classes = len(np.unique(y_training_mapped))  # The number of unique classes in your target variable\n",
    "dropout_rate = 0.2\n",
    "\n",
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # halving the size\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes) # Output size matches the number of classes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "epochs = 150\n",
    "# nn_model = SimpleNeuralNet()\n",
    "nn_model = SimpleNeuralNet(embedding_size, hidden_size, num_classes, dropout_rate)\n",
    "# the optimiser controls the learning rate\n",
    "# optimiser = torch.optim.SGD(nn_model.parameters(), lr=1e-2, momentum=0)\n",
    "\n",
    "# optimiser with weight decay for L2 regularization\n",
    "# The weight_decay parameter is used to specify the L2 penalty\n",
    "optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "\n",
    "# Adam without L2 regularisation\n",
    "# optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3) \n",
    "\n",
    "'''\n",
    "The L2 regularization penalty works by adding a term to the loss that penalizes large weights, which helps prevent overfitting. \n",
    "This term is the sum of the squares of all the weights multiplied by the weight_decay value. During backpropagation, \n",
    "this has the effect of shrinking the weights slightly on every update, hence the term \"weight decay.\"\n",
    "\n",
    "Be cautious when combining L2 regularization with the Adam optimizer, as Adam already includes a form of regularization \n",
    "through its moving average of squared gradients (which is similar to RMSprop). Excessive regularization might lead to underfitting, \n",
    "so itâ€™s important to find a balance that works well with your data and model architecture.\n",
    "'''\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs + 1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = nn_model(X_training_w2v_tensor)\n",
    "\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_training_tensor.shape)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y_training_tensor)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "    # Print every 1000 epochs\n",
    "    if i % 10 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    y_prediction_logits = nn_model(X_validation_w2v_tensor)\n",
    "    y_prediction_classes = torch.argmax(y_prediction_logits, dim=1)  # Convert logits to class labels\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_validation_numpy = y_validation_tensor.numpy() - 1\n",
    "y_prediction_numpy = y_prediction_classes.numpy() - 1\n",
    "\n",
    "print(\"Validation data evaluated\")\n",
    "\n",
    "# print(y_validation_numpy)\n",
    "# print(y_prediction_numpy)\n",
    "\n",
    "print(classification_report(y_validation_numpy, y_prediction_numpy))\n",
    "print(f1_score(y_validation_numpy, y_prediction_numpy, average='macro'))\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "#print(classification_report(y_validation.numpy(), y_prediction.numpy()))\n",
    "\n",
    "#f1_score(y_validation, y_prediction, average='macro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('test.csv')\n",
    "X_test = test['Text']\n",
    "\n",
    "# X_test_bow = bow_vectorizer.transform(X_test)\n",
    "X_test_to_vec = np.array([sentence_to_vec(sentence, word_vectors) for sentence in X_test])\n",
    "# X_test_bow_tensor = torch.FloatTensor(X_test_bow.toarray())\n",
    "X_test_w2v_tensor = torch.FloatTensor(X_test_to_vec)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # y_testing_logits = nn_model(X_test_bow_tensor)\n",
    "    y_testing_logits = nn_model(X_test_w2v_tensor)\n",
    "    y_testing_classes = torch.argmax(y_testing_logits, dim=1) \n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_test_numpy = y_testing_classes.numpy() - 1\n",
    "\n",
    "test['Verdict'] = pd.Series(y_test_numpy)\n",
    "test.drop(columns=['Text'], inplace=True)\n",
    "test.to_csv('Project simple neural network.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
