{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "minmax_scaler = MinMaxScaler()\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for preprocessing\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join tokens back into a string\n",
    "    # text = ' '.join(tokens)\n",
    "    # but better to lemmatize\n",
    "    #text = [token.lemma_ for token in nlp(text)] #spacy\n",
    "    text = tokens\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis with Textblob + Frequency of each POS tag in a sentence with SpaCy + Sentence length + Ratio of unique words to length of sentence with SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_textblob(X_training):\n",
    "    \n",
    "    list_of_features = []\n",
    "\n",
    "    for list_of_tokens in X_training:\n",
    "\n",
    "        # Join tokens back into a single string\n",
    "        sentence = ' '.join(list_of_tokens)\n",
    "\n",
    "        # Analyze sentiment with TextBlob\n",
    "        blob = TextBlob(sentence)\n",
    "        sentiment_polarity = blob.sentiment.polarity\n",
    "        sentiment_subjectivity = blob.sentiment.subjectivity\n",
    "\n",
    "        # Combine sentence length, POS tag counts, and sentiment features into a single feature array for this sentence\n",
    "        features = [sentiment_polarity, sentiment_subjectivity]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(X_training):\n",
    "\n",
    "    pos_tags_of_interest = ['NOUN', 'VERB', 'ADJ', 'ADV']  # nouns, verbs, adjectives, adverbs\n",
    "    list_of_features = []\n",
    "\n",
    "    # Convert list of tokens back into sentences\n",
    "    sentences = [' '.join(list_of_tokens) for list_of_tokens in X_training]\n",
    "\n",
    "    for doc in nlp.pipe(sentences):\n",
    "    # for doc in nlp.pipe(sentences, disable=[\"ner\", \"parser\"]):  # only tagger component is needed\n",
    "        # Count the occurrences of each POS tag of interest\n",
    "        pos_counts = {tag: 0 for tag in pos_tags_of_interest}\n",
    "        for token in doc:\n",
    "            if token.pos_ in pos_tags_of_interest:\n",
    "                pos_counts[token.pos_] += 1 \n",
    "\n",
    "        \n",
    "        # Calculate unique words vs sentence length ratio\n",
    "        unique_words_ratio = len(set(token.text for token in doc)) / len(doc) if doc else 0\n",
    "\n",
    "        # Calculate sentence length\n",
    "        sentence_length = len(doc)\n",
    "\n",
    "        # Combine sentence length and POS tag counts into a single feature array for this sentence\n",
    "        features = [unique_words_ratio] + [sentence_length] + [pos_counts[tag] for tag in pos_tags_of_interest]\n",
    "        list_of_features.append(features)\n",
    "\n",
    "    # Convert the list of features into a numpy array\n",
    "    features_array = np.array(list_of_features)\n",
    "\n",
    "    # Scale the features using MinMaxScaler\n",
    "    minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    features_scaled = minmax_scaler.fit_transform(features_array)\n",
    "\n",
    "    return features_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main neural network code -- Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete\n",
      "Splitting done\n",
      "Mapped class labels to start from 0 instead of 1\n",
      "Sentiment analysis completed\n",
      "Feature engineering completed\n",
      "Converted sentences to vectors\n",
      "(39083, 300)\n",
      "(39083, 6)\n",
      "(39083, 2)\n",
      "Integrated feature engineering into the dataset\n",
      "Converted vectors to tensors\n",
      "Number of unique classes: 4\n",
      "Epoch\tLoss\t\n",
      "-----\t----\n",
      "   10\t1.3080294132232666\n",
      "   20\t1.1031100749969482\n",
      "   30\t0.8303283452987671\n",
      "   40\t0.6381742358207703\n",
      "   50\t0.5139918327331543\n",
      "   60\t0.42740750312805176\n",
      "   70\t0.3725055158138275\n",
      "   80\t0.33087286353111267\n",
      "   90\t0.29380762577056885\n",
      "  100\t0.26402580738067627\n",
      "  110\t0.24643246829509735\n",
      "  120\t0.2280084639787674\n",
      "  130\t0.21397952735424042\n",
      "  140\t0.20103852450847626\n",
      "  150\t0.19210505485534668\n",
      "Validation data evaluated\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.72      0.82      2793\n",
      "           1       0.94      0.80      0.87      1371\n",
      "           2       0.75      0.98      0.85      3587\n",
      "           3       0.86      0.76      0.81      2020\n",
      "\n",
      "    accuracy                           0.83      9771\n",
      "   macro avg       0.88      0.82      0.83      9771\n",
      "weighted avg       0.86      0.83      0.83      9771\n",
      "\n",
      "0.8348546585086525\n"
     ]
    }
   ],
   "source": [
    "# def sentence_to_vec(sentence, model):\n",
    "#     vecs = []\n",
    "#     for word in sentence.split():\n",
    "#         if word in model.key_to_index:\n",
    "#             vecs.append(model[word])\n",
    "#     if len(vecs) == 0:\n",
    "#         return np.zeros(model.vector_size)\n",
    "#     vecs = np.array(vecs)\n",
    "#     return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "def list_of_tokens_to_vec(list_of_tokens, model):\n",
    "    vecs = []\n",
    "    for word in list_of_tokens:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv('D:/!Education/CS4248/Project/fulltrain.csv')\n",
    "X_train = train['Text']\n",
    "y_train = train['Label']\n",
    "\n",
    "preprocessed_train = [preprocess_text(text) for text in X_train]\n",
    "X_train_preprocessed = preprocessed_train\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Splitting done\")\n",
    "\n",
    "# Map class labels to start from 0 to fit NN model\n",
    "y_training_mapped = y_training - 1\n",
    "y_validation_mapped = y_validation - 1\n",
    "print(\"Mapped class labels to start from 0 instead of 1\")\n",
    "\n",
    "# bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "# X_training_bow = bow_vectorizer.fit_transform(X_training)\n",
    "# X_validation_bow = bow_vectorizer.transform(X_validation)\n",
    "\n",
    "X_training_with_sentiment = sentiment_analysis_textblob(X_training)\n",
    "X_validation_with_sentiment = sentiment_analysis_textblob(X_validation)\n",
    "print(\"Sentiment analysis completed\")\n",
    "\n",
    "X_training_engineered = feature_engineering(X_training)\n",
    "X_validation_engineered = feature_engineering(X_validation)\n",
    "print(\"Feature engineering completed\")\n",
    "\n",
    "X_training_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_training])\n",
    "X_validation_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_validation])\n",
    "print(\"Converted sentences to vectors\")\n",
    "\n",
    "print(X_training_w2v.shape)  # Should be (39083, num_features_w2v)\n",
    "print(X_training_engineered.shape)  # Should be (39083, num_features_engineered)\n",
    "print(X_training_with_sentiment.shape)  # Should be (39083, num_features_sentiment)\n",
    "\n",
    "X_training_combined = np.hstack([X_training_w2v, X_training_engineered, X_training_with_sentiment])\n",
    "X_validation_combined = np.hstack([X_validation_w2v, X_validation_engineered, X_validation_with_sentiment])\n",
    "print(\"Integrated feature engineering into the dataset\")\n",
    "\n",
    "# convert sparse matrices returned by CountVectorizer into dense matrices before converting them into PyTorch tensors\n",
    "# X_training_bow_tensor = torch.FloatTensor(X_training_bow.toarray())\n",
    "# X_training_w2v_tensor = torch.FloatTensor(X_training_w2v)\n",
    "X_training_w2v_tensor = torch.FloatTensor(X_training_combined)\n",
    "\n",
    "# X_validation_bow_tensor = torch.FloatTensor(X_validation_bow.toarray())\n",
    "# X_validation_w2v_tensor = torch.FloatTensor(X_validation_w2v)\n",
    "X_validation_w2v_tensor = torch.FloatTensor(X_validation_combined)\n",
    "\n",
    "y_training_tensor = torch.LongTensor(y_training_mapped)\n",
    "# y_training_tensor = torch.LongTensor(y_training)\n",
    "y_validation_tensor = torch.LongTensor(y_validation_mapped.to_numpy())\n",
    "# y_validation_tensor = torch.LongTensor(y_validation.to_numpy())\n",
    "print(\"Converted vectors to tensors\")\n",
    "\n",
    "# num_features = X_training_bow.shape[1]\n",
    "embedding_size = word_vectors.vector_size + 8  # This should be 300 for Google News Word2Vec + 8 additional features\n",
    "hidden_size = 512\n",
    "num_classes = len(np.unique(y_training_mapped))  # The number of unique classes in target variable\n",
    "# num_classes = len(np.unique(y_training))  # The number of unique classes in target variable\n",
    "dropout_rate = 0.2\n",
    "\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # halving the size\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes) # Output size matches the number of classes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "epochs = 150\n",
    "# nn_model = SimpleNeuralNet()\n",
    "nn_model = SimpleNeuralNet(embedding_size, hidden_size, num_classes, dropout_rate)\n",
    "# the optimiser controls the learning rate\n",
    "# optimiser = torch.optim.SGD(nn_model.parameters(), lr=1e-2, momentum=0)\n",
    "\n",
    "# optimiser with weight decay for L2 regularization\n",
    "# The weight_decay parameter is used to specify the L2 penalty\n",
    "optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "\n",
    "# Adam without L2 regularisation\n",
    "# optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "'''\n",
    "The L2 regularization penalty works by adding a term to the loss that penalizes large weights, which helps prevent overfitting. \n",
    "This term is the sum of the squares of all the weights multiplied by the weight_decay value. During backpropagation, \n",
    "this has the effect of shrinking the weights slightly on every update, hence the term \"weight decay.\"\n",
    "\n",
    "Be cautious when combining L2 regularization with the Adam optimizer, as Adam already includes a form of regularization \n",
    "through its moving average of squared gradients (which is similar to RMSprop). Excessive regularization might lead to underfitting, \n",
    "so it’s important to find a balance that works well with your data and model architecture.\n",
    "'''\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs + 1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = nn_model(X_training_w2v_tensor)\n",
    "\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_training_tensor.shape)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y_training_tensor)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "    # Print every 1000 epochs\n",
    "    if i % 10 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    y_prediction_logits = nn_model(X_validation_w2v_tensor)\n",
    "    y_prediction_classes = torch.argmax(y_prediction_logits, dim=1)  # Convert logits to class labels\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_validation_numpy = y_validation_tensor.numpy()\n",
    "y_prediction_numpy = y_prediction_classes.numpy()\n",
    "\n",
    "print(\"Validation data evaluated\")\n",
    "\n",
    "# print(y_validation_numpy)\n",
    "# print(y_prediction_numpy)\n",
    "\n",
    "print(classification_report(y_validation_numpy, y_prediction_numpy))\n",
    "print(f1_score(y_validation_numpy, y_prediction_numpy, average='macro'))\n",
    "\n",
    "# Evaluate the model\n",
    "#print(classification_report(y_validation.numpy(), y_prediction.numpy()))\n",
    "\n",
    "#f1_score(y_validation, y_prediction, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model with balancedtest.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get the prediction for the test set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/!Education/CS4248/Project/4248_project_Dissect_LUN/dataset/balancedtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m y_test \u001b[38;5;241m=\u001b[39m test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('D:/!Education/CS4248/Project/4248_project_Dissect_LUN/dataset/balancedtest.csv')\n",
    "X_test = test['Text']\n",
    "y_test = test['Label']\n",
    "\n",
    "\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "X_test_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_test_preprocessed])\n",
    "X_test_with_sentiment = sentiment_analysis_textblob(X_test_preprocessed)\n",
    "X_test_engineered = feature_engineering(X_test_preprocessed)\n",
    "\n",
    "print(X_test_w2v.shape)\n",
    "print(X_test_engineered.shape)\n",
    "print(X_test_with_sentiment.shape)\n",
    "\n",
    "X_test_combined = np.hstack([X_test_w2v, X_test_engineered, X_test_with_sentiment])\n",
    "X_test_tensor = torch.FloatTensor(X_test_combined)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # y_testing_logits = nn_model(X_test_bow_tensor)\n",
    "    y_testing_logits = nn_model(X_test_tensor)\n",
    "    y_testing_classes = torch.argmax(y_testing_logits, dim=1) \n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "# Convert 'y_test' to integers, then to a NumPy array\n",
    "y_test_true_labels_numpy = y_test.astype(int).to_numpy() # true labels are 1, 2, 3, 4\n",
    "y_test_prediction_numpy = y_testing_classes.numpy() + 1 # predicted labels are 0, 1, 2, 3 -- need to add 1\n",
    "\n",
    "print(classification_report(y_test_true_labels_numpy, y_test_prediction_numpy))\n",
    "print(f1_score(y_test_true_labels_numpy, y_test_prediction_numpy, average='macro'))\n",
    "\n",
    "# test['Verdict'] = pd.Series(y_test_numpy)\n",
    "# test.drop(columns=['Text'], inplace=True)\n",
    "# test.to_csv('Project simple neural network.csv', index=False)\n",
    "\n",
    "# Create a new DataFrame with the necessary information\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': X_test,  # Assuming 'Text' is the column containing the text data\n",
    "    'Original Label': y_test,  # Assuming 'Label' contains the original labels\n",
    "    'Predicted Label': y_test_prediction_numpy\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete\n",
      "Splitting done\n",
      "Mapped class labels to start from 0 instead of 1\n",
      "Sentiment analysis completed\n",
      "Feature engineering completed\n",
      "Converted sentences to vectors\n",
      "(47836, 300)\n",
      "(47836, 6)\n",
      "(47836, 2)\n",
      "Integrated feature engineering into the dataset\n",
      "Converted vectors to tensors\n",
      "Number of unique classes: 4\n",
      "Epoch\tLoss\t\n",
      "-----\t----\n",
      "   10\t1.3157390356063843\n",
      "   20\t0.9693562388420105\n",
      "   30\t0.6574960947036743\n",
      "   40\t0.5233080983161926\n",
      "   50\t0.43827539682388306\n",
      "   60\t0.3805628716945648\n",
      "   70\t0.3389802575111389\n",
      "   80\t0.31142109632492065\n",
      "   90\t0.28969696164131165\n",
      "  100\t0.27078738808631897\n",
      "  110\t0.2552439570426941\n",
      "  120\t0.24159997701644897\n",
      "  130\t0.23066191375255585\n",
      "  140\t0.21697601675987244\n",
      "  150\t0.20749744772911072\n",
      "Validation data evaluated\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83      2801\n",
      "           1       0.94      0.86      0.90      2793\n",
      "           2       0.75      0.97      0.85      3508\n",
      "           3       0.86      0.80      0.83      2857\n",
      "\n",
      "    accuracy                           0.85     11959\n",
      "   macro avg       0.87      0.84      0.85     11959\n",
      "weighted avg       0.87      0.85      0.85     11959\n",
      "\n",
      "0.8512867667718579\n"
     ]
    }
   ],
   "source": [
    "def list_of_tokens_to_vec(list_of_tokens, model):\n",
    "    vecs = []\n",
    "    for word in list_of_tokens:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv('D:/!Education/CS4248/Project/merged_final_df_with_topics_new.csv')\n",
    "X_train = train['text']\n",
    "y_train = train['label']\n",
    "\n",
    "preprocessed_train = [preprocess_text(text) for text in X_train]\n",
    "X_train_preprocessed = preprocessed_train\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Splitting done\")\n",
    "\n",
    "# Map class labels to start from 0 to fit NN model\n",
    "y_training_mapped = y_training - 1\n",
    "y_validation_mapped = y_validation - 1\n",
    "print(\"Mapped class labels to start from 0 instead of 1\")\n",
    "\n",
    "X_training_with_sentiment = sentiment_analysis_textblob(X_training)\n",
    "X_validation_with_sentiment = sentiment_analysis_textblob(X_validation)\n",
    "print(\"Sentiment analysis completed\")\n",
    "\n",
    "X_training_engineered = feature_engineering(X_training)\n",
    "X_validation_engineered = feature_engineering(X_validation)\n",
    "print(\"Feature engineering completed\")\n",
    "\n",
    "X_training_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_training])\n",
    "X_validation_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_validation])\n",
    "print(\"Converted sentences to vectors\")\n",
    "\n",
    "print(X_training_w2v.shape)  # Should be (39083, num_features_w2v)\n",
    "print(X_training_engineered.shape)  # Should be (39083, num_features_engineered)\n",
    "print(X_training_with_sentiment.shape)  # Should be (39083, num_features_sentiment)\n",
    "\n",
    "X_training_combined = np.hstack([X_training_w2v, X_training_engineered, X_training_with_sentiment])\n",
    "X_validation_combined = np.hstack([X_validation_w2v, X_validation_engineered, X_validation_with_sentiment])\n",
    "print(\"Integrated feature engineering into the dataset\")\n",
    "\n",
    "# convert sparse matrices returned by CountVectorizer into dense matrices before converting them into PyTorch tensors\n",
    "X_training_w2v_tensor = torch.FloatTensor(X_training_combined)\n",
    "\n",
    "X_validation_w2v_tensor = torch.FloatTensor(X_validation_combined)\n",
    "\n",
    "y_training_tensor = torch.LongTensor(y_training_mapped)\n",
    "y_validation_tensor = torch.LongTensor(y_validation_mapped.to_numpy())\n",
    "print(\"Converted vectors to tensors\")\n",
    "\n",
    "# num_features = X_training_bow.shape[1]\n",
    "embedding_size = word_vectors.vector_size + 8  # This should be 300 for Google News Word2Vec + 8 additional features\n",
    "hidden_size = 512\n",
    "num_classes = len(np.unique(y_training_mapped))  # The number of unique classes in target variable\n",
    "dropout_rate = 0.2\n",
    "\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # halving the size\n",
    "        self.layer3 = nn.Linear(hidden_size // 2, hidden_size // 4)\n",
    "        self.output_layer = nn.Linear(hidden_size // 4, num_classes) # Output size matches the number of classes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "epochs = 150\n",
    "# nn_model = SimpleNeuralNet()\n",
    "nn_model = SimpleNeuralNet(embedding_size, hidden_size, num_classes, dropout_rate)\n",
    "# the optimiser controls the learning rate\n",
    "\n",
    "# optimiser with weight decay for L2 regularization\n",
    "# The weight_decay parameter is used to specify the L2 penalty\n",
    "optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "\n",
    "# Adam without L2 regularisation\n",
    "# optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs + 1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = nn_model(X_training_w2v_tensor)\n",
    "\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_training_tensor.shape)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y_training_tensor)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "    # Print every 1000 epochs\n",
    "    if i % 10 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    y_prediction_logits = nn_model(X_validation_w2v_tensor)\n",
    "    y_prediction_classes = torch.argmax(y_prediction_logits, dim=1)  # Convert logits to class labels\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_validation_numpy = y_validation_tensor.numpy()\n",
    "y_prediction_numpy = y_prediction_classes.numpy()\n",
    "\n",
    "print(\"Validation data evaluated\")\n",
    "\n",
    "# print(y_validation_numpy)\n",
    "# print(y_prediction_numpy)\n",
    "\n",
    "print(classification_report(y_validation_numpy, y_prediction_numpy))\n",
    "print(f1_score(y_validation_numpy, y_prediction_numpy, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2996, 300)\n",
      "(2996, 6)\n",
      "(2996, 2)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.24      0.38       750\n",
      "           1       0.45      0.02      0.03       747\n",
      "           2       0.30      1.00      0.46       750\n",
      "           3       0.68      0.23      0.35       749\n",
      "\n",
      "    accuracy                           0.37      2996\n",
      "   macro avg       0.59      0.37      0.31      2996\n",
      "weighted avg       0.59      0.37      0.31      2996\n",
      "\n",
      "0.3059691595428497\n"
     ]
    }
   ],
   "source": [
    "# get the prediction for the test set\n",
    "test = pd.read_csv('D:/!Education/CS4248/Project/4248_project_Dissect_LUN/dataset/test_final_with_topics_new.csv')\n",
    "X_test = test['text']\n",
    "y_test = test['label']\n",
    "\n",
    "\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "X_test_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_test_preprocessed])\n",
    "X_test_with_sentiment = sentiment_analysis_textblob(X_test_preprocessed)\n",
    "X_test_engineered = feature_engineering(X_test_preprocessed)\n",
    "\n",
    "print(X_test_w2v.shape)\n",
    "print(X_test_engineered.shape)\n",
    "print(X_test_with_sentiment.shape)\n",
    "\n",
    "X_test_combined = np.hstack([X_test_w2v, X_test_engineered, X_test_with_sentiment])\n",
    "X_test_tensor = torch.FloatTensor(X_test_combined)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # y_testing_logits = nn_model(X_test_bow_tensor)\n",
    "    y_testing_logits = nn_model(X_test_tensor)\n",
    "    y_testing_classes = torch.argmax(y_testing_logits, dim=1) \n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "# Convert 'y_test' to integers, then to a NumPy array\n",
    "y_test_true_labels_numpy = y_test.astype(int).to_numpy() # true labels are 1, 2, 3, 4\n",
    "y_test_prediction_numpy = y_testing_classes.numpy() + 1 # predicted labels are 0, 1, 2, 3 -- need to add 1\n",
    "\n",
    "print(classification_report(y_test_true_labels_numpy, y_test_prediction_numpy))\n",
    "print(f1_score(y_test_true_labels_numpy, y_test_prediction_numpy, average='macro'))\n",
    "\n",
    "# test['Verdict'] = pd.Series(y_test_numpy)\n",
    "# test.drop(columns=['Text'], inplace=True)\n",
    "# test.to_csv('Project simple neural network.csv', index=False)\n",
    "\n",
    "# Create a new DataFrame with the necessary information\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': X_test,  # Assuming 'Text' is the column containing the text data\n",
    "    'Original Label': y_test,  # Assuming 'Label' contains the original labels\n",
    "    'Predicted Label': y_test_prediction_numpy\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a simpler neural network with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_tokens_to_vec(list_of_tokens, model):\n",
    "    vecs = []\n",
    "    for word in list_of_tokens:\n",
    "        if word in model.key_to_index:\n",
    "            vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    vecs = np.array(vecs)\n",
    "    return vecs.mean(axis=0)\n",
    "\n",
    "# load the data\n",
    "train = pd.read_csv('D:/!Education/CS4248/Project/fulltrain.csv')\n",
    "X_train = train['Text']\n",
    "y_train = train['Label']\n",
    "\n",
    "preprocessed_train = [preprocess_text(text) for text in X_train]\n",
    "X_train_preprocessed = preprocessed_train\n",
    "print(\"Preprocessing complete\")\n",
    "\n",
    "X_training, X_validation, y_training, y_validation = train_test_split(X_train_preprocessed, y_train, test_size=0.2, random_state=42)\n",
    "print(\"Splitting done\")\n",
    "\n",
    "# Map class labels to start from 0 to fit NN model\n",
    "y_training_mapped = y_training - 1\n",
    "y_validation_mapped = y_validation - 1\n",
    "print(\"Mapped class labels to start from 0 instead of 1\")\n",
    "\n",
    "X_training_with_sentiment = sentiment_analysis_textblob(X_training)\n",
    "X_validation_with_sentiment = sentiment_analysis_textblob(X_validation)\n",
    "print(\"Sentiment analysis completed\")\n",
    "\n",
    "X_training_engineered = feature_engineering(X_training)\n",
    "X_validation_engineered = feature_engineering(X_validation)\n",
    "print(\"Feature engineering completed\")\n",
    "\n",
    "X_training_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_training])\n",
    "X_validation_w2v = np.array([list_of_tokens_to_vec(list_of_tokens, word_vectors) for list_of_tokens in X_validation])\n",
    "print(\"Converted sentences to vectors\")\n",
    "\n",
    "print(X_training_w2v.shape)  # Should be (39083, num_features_w2v)\n",
    "print(X_training_engineered.shape)  # Should be (39083, num_features_engineered)\n",
    "print(X_training_with_sentiment.shape)  # Should be (39083, num_features_sentiment)\n",
    "\n",
    "X_training_combined = np.hstack([X_training_w2v, X_training_engineered, X_training_with_sentiment])\n",
    "X_validation_combined = np.hstack([X_validation_w2v, X_validation_engineered, X_validation_with_sentiment])\n",
    "print(\"Integrated feature engineering into the dataset\")\n",
    "\n",
    "# convert sparse matrices returned by CountVectorizer into dense matrices before converting them into PyTorch tensors\n",
    "X_training_w2v_tensor = torch.FloatTensor(X_training_combined)\n",
    "\n",
    "X_validation_w2v_tensor = torch.FloatTensor(X_validation_combined)\n",
    "\n",
    "y_training_tensor = torch.LongTensor(y_training_mapped)\n",
    "y_validation_tensor = torch.LongTensor(y_validation_mapped.to_numpy())\n",
    "print(\"Converted vectors to tensors\")\n",
    "\n",
    "# num_features = X_training_bow.shape[1]\n",
    "embedding_size = word_vectors.vector_size + 8  # This should be 300 for Google News Word2Vec + 8 additional features\n",
    "hidden_size = 512\n",
    "num_classes = len(np.unique(y_training_mapped))  # The number of unique classes in target variable\n",
    "dropout_rate = 0.2\n",
    "\n",
    "print(\"Number of unique classes:\", num_classes)\n",
    "\n",
    "\n",
    "class SimpleNeuralNet(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_classes, dropout_rate):\n",
    "        super(SimpleNeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(embedding_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size // 2)  # halving the size\n",
    "        self.output_layer = nn.Linear(hidden_size // 2, num_classes) # Output size matches the number of classes\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(6) # Set seed to some fixed value\n",
    "epochs = 150\n",
    "# nn_model = SimpleNeuralNet()\n",
    "nn_model = SimpleNeuralNet(embedding_size, hidden_size, num_classes, dropout_rate)\n",
    "# the optimiser controls the learning rate\n",
    "\n",
    "# optimiser with weight decay for L2 regularization\n",
    "# The weight_decay parameter is used to specify the L2 penalty\n",
    "optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3, weight_decay=1e-5) \n",
    "\n",
    "# Adam without L2 regularisation\n",
    "# optimiser = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Epoch', 'Loss', '\\n-----', '----', sep='\\t')\n",
    "for i in range(1, epochs + 1):\n",
    "    # reset gradients to 0\n",
    "    optimiser.zero_grad()\n",
    "    # get predictions\n",
    "    y_pred = nn_model(X_training_w2v_tensor)\n",
    "\n",
    "    # print(y_pred.shape)\n",
    "    # print(y_training_tensor.shape)\n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y_training_tensor)\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # update the model weights\n",
    "    optimiser.step()\n",
    "    # Print every 1000 epochs\n",
    "    if i % 10 == 0:\n",
    "        print (f\"{i:5d}\", loss.item(), sep='\\t')\n",
    "\n",
    "\n",
    "with torch.no_grad():  # No gradient computation for evaluation\n",
    "    y_prediction_logits = nn_model(X_validation_w2v_tensor)\n",
    "    y_prediction_classes = torch.argmax(y_prediction_logits, dim=1)  # Convert logits to class labels\n",
    "\n",
    "# Convert tensors to numpy arrays for sklearn functions\n",
    "y_validation_numpy = y_validation_tensor.numpy()\n",
    "y_prediction_numpy = y_prediction_classes.numpy()\n",
    "\n",
    "print(\"Validation data evaluated\")\n",
    "\n",
    "# print(y_validation_numpy)\n",
    "# print(y_prediction_numpy)\n",
    "\n",
    "print(classification_report(y_validation_numpy, y_prediction_numpy))\n",
    "print(f1_score(y_validation_numpy, y_prediction_numpy, average='macro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4248",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
