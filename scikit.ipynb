{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Notebook to train XGBoost model on both the original and augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure NLTK if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained from https://gist.github.com/susanli2016/d35def30b99f0e2f56c0e01e19ad0878\n",
    "def gettop_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(row):\n",
    "    return len(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on the dataset\n",
    "def feature_engineering(dataset, column_name):\n",
    "    dataset['los'] = dataset.apply(get_len, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>has_swear_word</th>\n",
       "      <th>severity</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['little', 'less', 'decade', 'ago', 'hockey', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['writers', 'hbo', 'series', 'sopranos', 'took...</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['despite', 'claims', 'tv', 'news', 'outlet', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['receiving', 'subpar', 'service', 'experienci...</td>\n",
       "      <td>4</td>\n",
       "      <td>3913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['watching', 'beloved', 'seattle', 'mariners',...</td>\n",
       "      <td>0</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>An estimated 300 naked women, including actres...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['estimated', '300', 'naked', 'women', 'includ...</td>\n",
       "      <td>0</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>Across the U.S., ceremonies have already begun...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['across', 'us', 'ceremonies', 'already', 'beg...</td>\n",
       "      <td>0</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "      <td>Describing himself as a complete anomaly withi...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['describing', 'complete', 'anomaly', 'within'...</td>\n",
       "      <td>0</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>In what some economists believe to be a sign t...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['economists', 'believe', 'sign', 'us', 'could...</td>\n",
       "      <td>4</td>\n",
       "      <td>3884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>WASHINGTON, DCWashington police seized 22 memb...</td>\n",
       "      <td>True</td>\n",
       "      <td>8.8</td>\n",
       "      <td>['washington', 'dcwashington', 'police', 'seiz...</td>\n",
       "      <td>1</td>\n",
       "      <td>5287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text  has_swear_word  \\\n",
       "0       1  A little less than a decade ago, hockey fans w...           False   \n",
       "1       1  The writers of the HBO series The Sopranos too...           False   \n",
       "2       1  Despite claims from the TV news outlet to offe...           False   \n",
       "3       1  After receiving 'subpar' service and experienc...           False   \n",
       "4       1  After watching his beloved Seattle Mariners pr...           False   \n",
       "..    ...                                                ...             ...   \n",
       "95      1  An estimated 300 naked women, including actres...            True   \n",
       "96      1  Across the U.S., ceremonies have already begun...           False   \n",
       "97      1  Describing himself as a complete anomaly withi...           False   \n",
       "98      1  In what some economists believe to be a sign t...           False   \n",
       "99      1  WASHINGTON, DCWashington police seized 22 memb...            True   \n",
       "\n",
       "    severity                                     processed_text  topic   los  \n",
       "0        0.0  ['little', 'less', 'decade', 'ago', 'hockey', ...      0   873  \n",
       "1        0.0  ['writers', 'hbo', 'series', 'sopranos', 'took...      0   715  \n",
       "2        0.0  ['despite', 'claims', 'tv', 'news', 'outlet', ...      0  4443  \n",
       "3        0.0  ['receiving', 'subpar', 'service', 'experienci...      4  3913  \n",
       "4        0.0  ['watching', 'beloved', 'seattle', 'mariners',...      0  1058  \n",
       "..       ...                                                ...    ...   ...  \n",
       "95       1.0  ['estimated', '300', 'naked', 'women', 'includ...      0   654  \n",
       "96       0.0  ['across', 'us', 'ceremonies', 'already', 'beg...      0   520  \n",
       "97       0.0  ['describing', 'complete', 'anomaly', 'within'...      0   866  \n",
       "98       0.0  ['economists', 'believe', 'sign', 'us', 'could...      4  3884  \n",
       "99       8.8  ['washington', 'dcwashington', 'police', 'seiz...      1  5287  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "train_original = pd.read_csv('D:/!Education/CS4248/Project/fulltrain.csv')\n",
    "train_augment = pd.read_csv('D:/!Education/CS4248/Project/merged_final_df_with_topics.csv')\n",
    "X_train = feature_engineering(train_augment, ')')['los'].to_frame()\n",
    "\n",
    "# # Define the header\n",
    "# header = ['Label']\n",
    "\n",
    "# # Read the contents of the file\n",
    "# with open('D:/!Education/CS4248/Project/fulltrain.csv', 'r', newline='') as csvfile1:\n",
    "#     rows = list(csv.reader(csvfile1))\n",
    "\n",
    "# # Add the header\n",
    "# rows.insert(0, header)\n",
    "\n",
    "# # Write back to the CSV file\n",
    "# with open('D:/!Education/CS4248/Project/fulltrain.csv', 'w', newline='') as csvfile1:\n",
    "#     csvwriter = csv.writer(csvfile1)\n",
    "#     csvwriter.writerows(rows)\n",
    "\n",
    "\n",
    "\n",
    "# # Read the contents of the file\n",
    "# with open('D:/!Education/CS4248/Project/merged_final_df_with_topics.csv', 'r', newline='') as csvfile2:\n",
    "#     rows = list(csv.reader(csvfile2))\n",
    "\n",
    "# # Add the header\n",
    "# rows.insert(0, header)\n",
    "\n",
    "# # Write back to the CSV file\n",
    "# with open('D:/!Education/CS4248/Project/merged_final_df_with_topics.csv', 'w', newline='') as csvfile2:\n",
    "#     csvwriter = csv.writer(csvfile2)\n",
    "#     csvwriter.writerows(rows)\n",
    "\n",
    "\n",
    "y_train = train_original['Label']\n",
    "y_train = train_augment['label']\n",
    "train_augment[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        los\n",
      "0       873\n",
      "1       715\n",
      "2      4443\n",
      "3      3913\n",
      "4      1058\n",
      "...     ...\n",
      "59790  1996\n",
      "59791  1146\n",
      "59792  1665\n",
      "59793  1138\n",
      "59794  3296\n",
      "\n",
      "[59795 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Run this if you need to modify X_train again for some reason\n",
    "# X_train = train_augment['topic']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5086141748958363"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "X_train_formatted = np.array(X_train).reshape(-1, 1)\n",
    "# X_train_formatted = np.array(X_train)\n",
    "model.fit(X_train_formatted, y_train)\n",
    "y_pred = model.predict(X_train_formatted)\n",
    "f1_score(y_train, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "As a baseline for the \"random\" f1 score for the original dataset, I trained the XGBoost model on an X_train that was just the length of the text string. This f1 score turned out to be 0.5234, which is surprisingly high and not much less than the f1 score in the paper.\n",
    "Doing the same for the augmented dataset yields an F1 score of 0.5086, which is lower but is expected as the length of the text string should theoretically have zero correlation with the label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
