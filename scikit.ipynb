{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Notebook to train XGBoost model on both the original and augmented dataset\n",
    "\n",
    "Here, we use the xgboost package but use the classifier built in the package that is built to integrate with sklearn at the cost of some functionality.\n",
    "[Source](https://www.datacamp.com/tutorial/xgboost-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure NLTK if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained from https://gist.github.com/susanli2016/d35def30b99f0e2f56c0e01e19ad0878\n",
    "def gettop_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(row):\n",
    "    return len(row['Text'])\n",
    "\n",
    "def get_len_aug(row):\n",
    "    return len(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on the dataset\n",
    "def feature_engineering(dataset, aug):\n",
    "    if (aug):\n",
    "        dataset['los'] = dataset.apply(get_len_aug, axis=1)\n",
    "    else:\n",
    "        dataset['los'] = dataset.apply(get_len, axis=1)\n",
    "\n",
    "    # bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "    # col_name = \"\"\n",
    "    # if aug:\n",
    "    #     col_name = 'text'\n",
    "    # else:\n",
    "    #     col_name = 'Text'\n",
    "    # text_col = dataset[col_name]\n",
    "    # new_col = bow_vectorizer.fit_transform(text_col)\n",
    "    # dataset['bowvec'] = new_col\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>has_swear_word</th>\n",
       "      <th>severity</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['little', 'less', 'decade', 'ago', 'hockey', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['writers', 'hbo', 'series', 'sopranos', 'took...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['despite', 'claims', 'tv', 'news', 'outlet', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['receiving', 'subpar', 'service', 'experienci...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['watching', 'beloved', 'seattle', 'mariners',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>At a cafeteria-table press conference Monday, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['cafeteriatable', 'press', 'conference', 'mon...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Stunned shock and dismay were just a few of th...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['stunned', 'shock', 'dismay', 'reactions', 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Speaking with reporters before a game Monday, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['speaking', 'reporters', 'game', 'monday', 'l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Sports journalists and television crews were p...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['sports', 'journalists', 'television', 'crews...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>SALEM, VAF;or the eighth straight world-histor...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['salem', 'vafor', 'eighth', 'straight', 'worl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  has_swear_word  \\\n",
       "0      1  A little less than a decade ago, hockey fans w...           False   \n",
       "1      1  The writers of the HBO series The Sopranos too...           False   \n",
       "2      1  Despite claims from the TV news outlet to offe...           False   \n",
       "3      1  After receiving 'subpar' service and experienc...           False   \n",
       "4      1  After watching his beloved Seattle Mariners pr...           False   \n",
       "5      1  At a cafeteria-table press conference Monday, ...           False   \n",
       "6      1  Stunned shock and dismay were just a few of th...           False   \n",
       "7      1  Speaking with reporters before a game Monday, ...            True   \n",
       "8      1  Sports journalists and television crews were p...           False   \n",
       "9      1  SALEM, VAF;or the eighth straight world-histor...           False   \n",
       "\n",
       "   severity                                     processed_text  topic  \n",
       "0       0.0  ['little', 'less', 'decade', 'ago', 'hockey', ...      0  \n",
       "1       0.0  ['writers', 'hbo', 'series', 'sopranos', 'took...      0  \n",
       "2       0.0  ['despite', 'claims', 'tv', 'news', 'outlet', ...      0  \n",
       "3       0.0  ['receiving', 'subpar', 'service', 'experienci...      4  \n",
       "4       0.0  ['watching', 'beloved', 'seattle', 'mariners',...      0  \n",
       "5       0.0  ['cafeteriatable', 'press', 'conference', 'mon...      4  \n",
       "6       0.0  ['stunned', 'shock', 'dismay', 'reactions', 'b...      0  \n",
       "7       1.0  ['speaking', 'reporters', 'game', 'monday', 'l...      0  \n",
       "8       0.0  ['sports', 'journalists', 'television', 'crews...      0  \n",
       "9       0.0  ['salem', 'vafor', 'eighth', 'straight', 'worl...      0  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original = pd.read_csv('dataset/fulltrain.csv')\n",
    "train_augment = pd.read_csv('dataset/merged_final_df_with_topics.csv')\n",
    "# X = feature_engineering(train_augment, aug=True)['los'].to_frame()\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "X = bow_vectorizer.fit_transform(train_augment['text'])\n",
    "\n",
    "y = train_original['Label']\n",
    "# Comment this out if we are just using the original dataset\n",
    "y = train_augment['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "train_augment[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 140220)\t1\n",
      "  (0, 98223)\t2\n",
      "  (0, 59205)\t1\n",
      "  (0, 44663)\t1\n",
      "  (0, 170333)\t1\n",
      "  (0, 167613)\t1\n",
      "  (0, 102293)\t1\n",
      "  (0, 81264)\t1\n",
      "  (0, 163155)\t1\n",
      "  (0, 55795)\t1\n",
      "  (0, 224152)\t1\n",
      "  (0, 60277)\t1\n",
      "  (0, 12802)\t1\n",
      "  (0, 153431)\t1\n",
      "  (0, 130173)\t2\n",
      "  (0, 145188)\t1\n",
      "  (0, 124628)\t1\n",
      "  (0, 91928)\t1\n",
      "  (0, 191622)\t2\n",
      "  (0, 78997)\t1\n",
      "  (0, 124464)\t1\n",
      "  (0, 10156)\t1\n",
      "  (0, 50779)\t1\n",
      "  (0, 221905)\t1\n",
      "  (0, 154211)\t1\n",
      "  :\t:\n",
      "  (47835, 76236)\t3\n",
      "  (47835, 49310)\t1\n",
      "  (47835, 160789)\t1\n",
      "  (47835, 185804)\t1\n",
      "  (47835, 121871)\t1\n",
      "  (47835, 6562)\t2\n",
      "  (47835, 6934)\t1\n",
      "  (47835, 25588)\t2\n",
      "  (47835, 120446)\t1\n",
      "  (47835, 56900)\t1\n",
      "  (47835, 69594)\t1\n",
      "  (47835, 72703)\t2\n",
      "  (47835, 57801)\t2\n",
      "  (47835, 196753)\t1\n",
      "  (47835, 7593)\t1\n",
      "  (47835, 186782)\t1\n",
      "  (47835, 162230)\t1\n",
      "  (47835, 213581)\t1\n",
      "  (47835, 157287)\t1\n",
      "  (47835, 33923)\t1\n",
      "  (47835, 101862)\t1\n",
      "  (47835, 171282)\t1\n",
      "  (47835, 209486)\t2\n",
      "  (47835, 82674)\t1\n",
      "  (47835, 129886)\t1\n"
     ]
    }
   ],
   "source": [
    "#Run this if you need to modify X_train again for some reason\n",
    "# X_train = train_augment['topic']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GradientBoostingClassifier() # not xgboost, but just to test that a model works\n",
    "# X_train_formatted = np.array(X_train).reshape(-1, 1)\n",
    "# # X_train_formatted = np.array(X_train)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.8731360239806389\n",
      "accuracy score: 0.8759093569696463\n",
      "precision score: 0.8742293658809233\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n",
    "\n",
    "# The label encoder is necessary as XGBClassifier expects labels [0,1,2,3] but we have [1,2,3,4]\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "print(\"f1 score: \" + str(f1_score(y_test, y_pred, average='macro')))\n",
    "print(\"accuracy score: \" + str(accuracy_score(y_test, y_pred)))\n",
    "print(\"precision score: \" + str(precision_score(y_test, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "XGBoost classifier (Sklearn version)\n",
    "As a baseline for the \"random\" f1 score for the original dataset, I trained the XGBoost model on an X_train that was just the length of the text string. This f1 score turned out to be 0.06729. This is expected, and it just means that any meaningful features will produce an F1 score higher than this.<br>\n",
    "Doing the same for the augmented dataset yields an F1 score of 0.1285. This improvement does not necessarily mean that the augmented dataset is \"better\", but rather that this is the base that any meaningful feature needs to beat.\n",
    "\n",
    "We tried converting each of the text into a bag of words vector and training the XGB classifier on it. The f1 score obtained was 0.8765 for the original dataset, with an accuracy of 0.8851 and precision of 0.8935. Doing the same for the augmented dataset, the f1 score obtained was 0.8731, with accuracy 0.8759 and precision 0.8742. Although the model seems to do poorer on the augmented dataset, the discrepancy is minimal.<br>\n",
    "Here, I think it is safe to conclude that when it comes to this particular feature, adding the new rows to the dataset does not affect the performance of the XGBoost classifier model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
