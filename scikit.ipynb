{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Notebook to train XGBoost model on both the original and augmented dataset\n",
    "\n",
    "Here, we use the xgboost package but use the classifier built in the package that is built to integrate with sklearn at the cost of some functionality.\n",
    "[Source](https://www.datacamp.com/tutorial/xgboost-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure NLTK if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained from https://gist.github.com/susanli2016/d35def30b99f0e2f56c0e01e19ad0878\n",
    "def gettop_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(row):\n",
    "    return len(row['Text'])\n",
    "\n",
    "def get_len_aug(row):\n",
    "    return len(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on the dataset\n",
    "def feature_engineering(dataset, aug):\n",
    "    if (aug):\n",
    "        dataset['los'] = dataset.apply(get_len_aug, axis=1)\n",
    "    else:\n",
    "        dataset['los'] = dataset.apply(get_len, axis=1)\n",
    "\n",
    "    # bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "    # col_name = \"\"\n",
    "    # if aug:\n",
    "    #     col_name = 'text'\n",
    "    # else:\n",
    "    #     col_name = 'Text'\n",
    "    # text_col = dataset[col_name]\n",
    "    # new_col = bow_vectorizer.fit_transform(text_col)\n",
    "    # dataset['bowvec'] = new_col\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>has_swear_word</th>\n",
       "      <th>severity</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['little', 'less', 'decade', 'ago', 'hockey', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['writers', 'hbo', 'series', 'sopranos', 'took...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['despite', 'claims', 'tv', 'news', 'outlet', ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['receiving', 'subpar', 'service', 'experienci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['watching', 'beloved', 'seattle', 'mariners',...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>At a cafeteria-table press conference Monday, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['cafeteriatable', 'press', 'conference', 'mon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Stunned shock and dismay were just a few of th...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['stunned', 'shock', 'dismay', 'reactions', 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Speaking with reporters before a game Monday, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['speaking', 'reporters', 'game', 'monday', 'l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Sports journalists and television crews were p...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['sports', 'journalists', 'television', 'crews...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>SALEM, VAF;or the eighth straight world-histor...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['salem', 'vafor', 'eighth', 'straight', 'worl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  has_swear_word  \\\n",
       "0      1  A little less than a decade ago, hockey fans w...           False   \n",
       "1      1  The writers of the HBO series The Sopranos too...           False   \n",
       "2      1  Despite claims from the TV news outlet to offe...           False   \n",
       "3      1  After receiving 'subpar' service and experienc...           False   \n",
       "4      1  After watching his beloved Seattle Mariners pr...           False   \n",
       "5      1  At a cafeteria-table press conference Monday, ...           False   \n",
       "6      1  Stunned shock and dismay were just a few of th...           False   \n",
       "7      1  Speaking with reporters before a game Monday, ...            True   \n",
       "8      1  Sports journalists and television crews were p...           False   \n",
       "9      1  SALEM, VAF;or the eighth straight world-histor...           False   \n",
       "\n",
       "   severity                                     processed_text  topic  \n",
       "0       0.0  ['little', 'less', 'decade', 'ago', 'hockey', ...      0  \n",
       "1       0.0  ['writers', 'hbo', 'series', 'sopranos', 'took...      4  \n",
       "2       0.0  ['despite', 'claims', 'tv', 'news', 'outlet', ...      4  \n",
       "3       0.0  ['receiving', 'subpar', 'service', 'experienci...      0  \n",
       "4       0.0  ['watching', 'beloved', 'seattle', 'mariners',...      0  \n",
       "5       0.0  ['cafeteriatable', 'press', 'conference', 'mon...      0  \n",
       "6       0.0  ['stunned', 'shock', 'dismay', 'reactions', 'b...      0  \n",
       "7       1.0  ['speaking', 'reporters', 'game', 'monday', 'l...      0  \n",
       "8       0.0  ['sports', 'journalists', 'television', 'crews...      0  \n",
       "9       0.0  ['salem', 'vafor', 'eighth', 'straight', 'worl...      0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original = pd.read_csv('dataset/fulltrain.csv')\n",
    "train_augment = pd.read_csv('dataset/merged_final_df_with_topics_new.csv')\n",
    "test_original = pd.read_csv('dataset/balancedtest.csv')\n",
    "test_augment = pd.read_csv('dataset/test_final_with_topics_new.csv')\n",
    "\n",
    "bow_vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "# X_ori = bow_vectorizer.fit_transform(train_original['Text'])\n",
    "# X_aug = bow_vectorizer.fit_transform(train_augment['text'])\n",
    "\n",
    "# y_ori = train_original['Label']\n",
    "# y_aug = train_augment['label']\n",
    "\n",
    "X_train_ori = bow_vectorizer.fit_transform(train_original['Text'])\n",
    "X_train_aug = bow_vectorizer.fit_transform(train_augment['text'])\n",
    "\n",
    "y_train_ori = train_original['Label']\n",
    "y_train_aug = train_augment['label']\n",
    "\n",
    "X_test_ori = bow_vectorizer.fit_transform(test_original['Text'])\n",
    "X_test_aug = bow_vectorizer.fit_transform(test_augment['text'])\n",
    "\n",
    "y_test_ori = test_original['Label']\n",
    "y_test_aug = test_augment['label']\n",
    "\n",
    "# X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(X_ori, y_ori, test_size=0.20, random_state=42)\n",
    "# X_train_aug, X_test_aug, y_train_aug, y_test_aug = train_test_split(X_aug, y_aug, test_size=0.20, random_state=42)\n",
    "train_augment[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label                                               Text\n",
      "0      1  A little less than a decade ago, hockey fans w...\n",
      "1      1  The writers of the HBO series The Sopranos too...\n",
      "2      1  Despite claims from the TV news outlet to offe...\n",
      "3      1  After receiving 'subpar' service and experienc...\n",
      "4      1  After watching his beloved Seattle Mariners pr...\n",
      "   Label                                               Text\n",
      "0      1  When so many actors seem content to churn out ...\n",
      "1      1   In what football insiders are calling an unex...\n",
      "2      1  In a freak accident following Game 3 of the N....\n",
      "3      1  North Koreas official news agency announced to...\n",
      "4      1  The former Alaska Governor Sarah Palin would b...\n"
     ]
    }
   ],
   "source": [
    "#Run this if you need to modify X_train again for some reason\n",
    "# X_train = train_augment['topic']\n",
    "# print(X_train_ori.shape)\n",
    "# print(X_test_ori.shape)\n",
    "print(train_original[:5])\n",
    "print(test_original[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = GradientBoostingClassifier() # not xgboost, but just to test that a model works\n",
    "# X_train_formatted = np.array(X_train).reshape(-1, 1)\n",
    "# # X_train_formatted = np.array(X_train)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 229285, got 47659",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Original dataset\u001b[39;00m\n\u001b[1;32m     11\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(X_train_ori, y_train_ori)\n\u001b[0;32m---> 12\u001b[0m y_pred_ori \u001b[38;5;241m=\u001b[39m \u001b[43mxgb_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_ori\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal dataset:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(classification_report(y_test_ori, y_pred_ori))\n",
      "File \u001b[0;32m~/Desktop/Year_4_Sem_2/CS4248/4248_project_Dissect_LUN/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1553\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1546\u001b[0m     X: ArrayLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1551\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[0;32m-> 1553\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[1;32m   1561\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[0;32m~/Desktop/Year_4_Sem_2/CS4248/4248_project_Dissect_LUN/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1168\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1177\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Year_4_Sem_2/CS4248/4248_project_Dissect_LUN/.venv/lib/python3.10/site-packages/xgboost/core.py:2428\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2425\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2426\u001b[0m         )\n\u001b[1;32m   2427\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2428\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2429\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2430\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2431\u001b[0m         )\n\u001b[1;32m   2433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_np_array_like(data):\n\u001b[1;32m   2434\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ensure_np_dtype\n",
      "\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 229285, got 47659"
     ]
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n",
    "\n",
    "# The label encoder is necessary as XGBClassifier expects labels [0,1,2,3] but we have [1,2,3,4]\n",
    "le = LabelEncoder()\n",
    "y_train_ori = le.fit_transform(y_train_ori)\n",
    "y_test_ori = le.fit_transform(y_test_ori)\n",
    "y_train_aug = le.fit_transform(y_train_aug)\n",
    "y_test_aug = le.fit_transform(y_test_aug)\n",
    "\n",
    "# Original dataset\n",
    "xgb_classifier.fit(X_train_ori, y_train_ori)\n",
    "y_pred_ori = xgb_classifier.predict(X_train_ori)\n",
    "print('Original dataset:\\n')\n",
    "print(classification_report(y_test_ori, y_pred_ori))\n",
    "\n",
    "# Augmented dataset\n",
    "xgb_classifier.fit(X_train_aug, y_train_aug)\n",
    "y_pred_aug = xgb_classifier.predict(X_test_aug)\n",
    "print('Augmented dataset:\\n')\n",
    "print(classification_report(y_test_aug, y_pred_aug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "XGBoost classifier (Sklearn version)\n",
    "As a baseline for the \"random\" f1 score for the original dataset, I trained the XGBoost model on an X_train that was just the length of the text string. This f1 score turned out to be 0.06729. This is expected, and it just means that any meaningful features will produce an F1 score higher than this.<br>\n",
    "Doing the same for the augmented dataset yields an F1 score of 0.1285. This improvement does not necessarily mean that the augmented dataset is \"better\", but rather that this is the base that any meaningful feature needs to beat.\n",
    "\n",
    "We tried converting each of the text into a bag of words vector and training the XGB classifier on it. The f1 score obtained was 0.8765 for the original dataset, with an accuracy of 0.8851 and precision of 0.8935. Doing the same for the augmented dataset, the f1 score obtained was 0.8731, with accuracy 0.8759 and precision 0.8742. Although the model seems to do poorer on the augmented dataset, the discrepancy is minimal.<br>\n",
    "Here, I think it is safe to conclude that when it comes to this particular feature, adding the new rows to the dataset does not affect the performance of the XGBoost classifier model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
