{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Notebook to train XGBoost model on both the original and augmented dataset\n",
    "\n",
    "Here, we use the xgboost package but use the classifier built in the package that is built to integrate with sklearn at the cost of some functionality.\n",
    "[Source](https://www.datacamp.com/tutorial/xgboost-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure NLTK if applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained from https://gist.github.com/susanli2016/d35def30b99f0e2f56c0e01e19ad0878\n",
    "def gettop_n_bigram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_len(row):\n",
    "    return len(row['Text'])\n",
    "\n",
    "def get_len_aug(row):\n",
    "    return len(row['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform feature engineering on the dataset\n",
    "def feature_engineering(dataset, aug):\n",
    "    if (aug):\n",
    "        dataset['los'] = dataset.apply(get_len_aug, axis=1)\n",
    "    else:\n",
    "        dataset['los'] = dataset.apply(get_len, axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>has_swear_word</th>\n",
       "      <th>severity</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>topic</th>\n",
       "      <th>los</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A little less than a decade ago, hockey fans w...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['little', 'less', 'decade', 'ago', 'hockey', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The writers of the HBO series The Sopranos too...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['writers', 'hbo', 'series', 'sopranos', 'took...</td>\n",
       "      <td>0</td>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Despite claims from the TV news outlet to offe...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['despite', 'claims', 'tv', 'news', 'outlet', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>After receiving 'subpar' service and experienc...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['receiving', 'subpar', 'service', 'experienci...</td>\n",
       "      <td>4</td>\n",
       "      <td>3913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>After watching his beloved Seattle Mariners pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['watching', 'beloved', 'seattle', 'mariners',...</td>\n",
       "      <td>0</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>At a cafeteria-table press conference Monday, ...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['cafeteriatable', 'press', 'conference', 'mon...</td>\n",
       "      <td>4</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>Stunned shock and dismay were just a few of th...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['stunned', 'shock', 'dismay', 'reactions', 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Speaking with reporters before a game Monday, ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['speaking', 'reporters', 'game', 'monday', 'l...</td>\n",
       "      <td>0</td>\n",
       "      <td>3755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>Sports journalists and television crews were p...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['sports', 'journalists', 'television', 'crews...</td>\n",
       "      <td>0</td>\n",
       "      <td>955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>SALEM, VAF;or the eighth straight world-histor...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['salem', 'vafor', 'eighth', 'straight', 'worl...</td>\n",
       "      <td>0</td>\n",
       "      <td>544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  has_swear_word  \\\n",
       "0      1  A little less than a decade ago, hockey fans w...           False   \n",
       "1      1  The writers of the HBO series The Sopranos too...           False   \n",
       "2      1  Despite claims from the TV news outlet to offe...           False   \n",
       "3      1  After receiving 'subpar' service and experienc...           False   \n",
       "4      1  After watching his beloved Seattle Mariners pr...           False   \n",
       "5      1  At a cafeteria-table press conference Monday, ...           False   \n",
       "6      1  Stunned shock and dismay were just a few of th...           False   \n",
       "7      1  Speaking with reporters before a game Monday, ...            True   \n",
       "8      1  Sports journalists and television crews were p...           False   \n",
       "9      1  SALEM, VAF;or the eighth straight world-histor...           False   \n",
       "\n",
       "   severity                                     processed_text  topic   los  \n",
       "0       0.0  ['little', 'less', 'decade', 'ago', 'hockey', ...      0   873  \n",
       "1       0.0  ['writers', 'hbo', 'series', 'sopranos', 'took...      0   715  \n",
       "2       0.0  ['despite', 'claims', 'tv', 'news', 'outlet', ...      0  4443  \n",
       "3       0.0  ['receiving', 'subpar', 'service', 'experienci...      4  3913  \n",
       "4       0.0  ['watching', 'beloved', 'seattle', 'mariners',...      0  1058  \n",
       "5       0.0  ['cafeteriatable', 'press', 'conference', 'mon...      4   600  \n",
       "6       0.0  ['stunned', 'shock', 'dismay', 'reactions', 'b...      0   756  \n",
       "7       1.0  ['speaking', 'reporters', 'game', 'monday', 'l...      0  3755  \n",
       "8       0.0  ['sports', 'journalists', 'television', 'crews...      0   955  \n",
       "9       0.0  ['salem', 'vafor', 'eighth', 'straight', 'worl...      0   544  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_original = pd.read_csv('dataset/fulltrain.csv')\n",
    "train_augment = pd.read_csv('dataset/merged_final_df_with_topics.csv')\n",
    "X = feature_engineering(train_augment, aug=True)['los'].to_frame()\n",
    "\n",
    "y = train_original['Label']\n",
    "# Comment this out if we are just using the original dataset\n",
    "y = train_augment['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "train_augment[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        los\n",
      "58515   844\n",
      "28095  9019\n",
      "2081   1077\n",
      "41524   970\n",
      "53155  7952\n",
      "...     ...\n",
      "54343   868\n",
      "38158  2630\n",
      "860    4033\n",
      "15795  5460\n",
      "56422   828\n",
      "\n",
      "[47836 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#Run this if you need to modify X_train again for some reason\n",
    "# X_train = train_augment['topic']\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5265790920526502"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = GradientBoostingClassifier() # not xgboost, but just to test that a model works\n",
    "# X_train_formatted = np.array(X_train).reshape(-1, 1)\n",
    "# # X_train_formatted = np.array(X_train)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12850733076987833"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n",
    "\n",
    "# The label encoder is necessary as XGBClassifier expects labels [0,1,2,3] but we have [1,2,3,4]\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "y_pred = xgb_classifier.predict(X_test)\n",
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "XGBoost classifier (Sklearn version)\n",
    "As a baseline for the \"random\" f1 score for the original dataset, I trained the XGBoost model on an X_train that was just the length of the text string. This f1 score turned out to be 0.06729. This is expected, and it just means that any meaningful features will produce an F1 score higher than this.<br>\n",
    "Doing the same for the augmented dataset yields an F1 score of 0.1285. This improvement does not necessarily mean that the augmented dataset is \"better\", but rather that this is the base that any meaningful feature needs to beat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
